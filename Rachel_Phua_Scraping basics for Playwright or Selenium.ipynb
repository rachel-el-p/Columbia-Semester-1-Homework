{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright or Selenium\n",
    "\n",
    "If you feel comfortable with scraping in general, you're free to skip this notebook and try to go right to the next one. Same thing if you get bored partway down.\n",
    "\n",
    "**Possibly useful links:**\n",
    "\n",
    "* Scraping section of my [everything page](https://jonathansoma.com/everything/)\n",
    "* Some [old Selenium snippets](http://jonathansoma.com/lede/foundations-2018/classes/selenium/selenium-snippets/) (if you decide to use Selenium)\n",
    "* [Loops in Playwright](https://jonathansoma.com/everything/scraping/loops-in-playwright/), which is the thing that we were having trouble with during class when using `.locator` so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports\n",
    "\n",
    "Import what you need to use Playwright or Selenium, and start up a new browser to use for scraping. \n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/rachelp/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages (1.28.0)\n",
      "Requirement already satisfied: greenlet==2.0.1 in /Users/rachelp/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages (from playwright) (2.0.1)\n",
      "Requirement already satisfied: pyee==9.0.4 in /Users/rachelp/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages (from playwright) (9.0.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/rachelp/.pyenv/versions/3.11.0rc2/lib/python3.11/site-packages (from pyee==9.0.4->playwright) (4.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install playwright\n",
    "!pip install playwright\n",
    "\n",
    "# Install a browser (Chromium, open-source Chrome)\n",
    "!playwright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = await playwright.chromium.launch(headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-class.html, printing out the title, subhead, and byline. You're welcome to use BeautifulSoup as long as the information comes from Playwright/Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/by-class.html' request=<Request url='https://jonathansoma.com/lede/static/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/by-class.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Some Supplemental Materials</h3>\\n<p class=\"byline\">By Jonathan Soma</p></body></html>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head><body><h1 class=\"title\">How to Scrape Things</h1>\n",
       "<h3 class=\"subhead\">Some Supplemental Materials</h3>\n",
       "<p class=\"byline\">By Jonathan Soma</p></body></html>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = await page.content()\n",
    "\n",
    "doc = BeautifulSoup(html)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scraping using tags\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-tag.html, printing out the title, subhead, and byline. You're welcome to use BeautifulSoup as long as the information comes from Playwright/Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Scrape Things\n",
      "Some Supplemental Materials\n",
      "By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "for row in doc:\n",
    "    title = row.select_one(\".title\").text\n",
    "    print(title)\n",
    "    subhead = row.select_one(\".subhead\").text\n",
    "    print(subhead)\n",
    "    \n",
    "    byline = row.select_one(\".byline\").text\n",
    "    print(byline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/by-list.html, printing out the title, subhead, and byline. You're welcome to use BeautifulSoup as long as the information comes from Playwright/Selenium.\n",
    "\n",
    "> **This will be important for the next few:** if you scrape multiple items, you have a list. In Selenium you can use `[0]`, `[1]`, `[-1]` etc just like you would for a normal list (and in Playwright, too, asl ong as you're using `query_selector_all`). If you're using locators you'll need to use `.nth(0)`, `nth(1)`, `nth(2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><p>How to Scrape Things</p>\\n<p>Some Supplemental Materials</p>\\n<p>By Jonathan Soma</p></body></html>'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "await page.goto(\"https://jonathansoma.com/lede/static/by-list.html\")\n",
    "await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Supplemental Materials'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_2 = await page.locator(\"p\").nth(1).text_content()\n",
    "doc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/single-table-row.html, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://jonathansoma.com/lede/static/single-table-row.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/lede/static/single-table-row.html' request=<Request url='https://jonathansoma.com/lede/static/single-table-row.html' method='GET'>>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "await page.goto(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Some Supplemental Materials</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                            1                 2\n",
       "0  How to Scrape Things  Some Supplemental Materials  By Jonathan Soma"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "tables = pd.read_html(html)\n",
    "tables\n",
    "df = tables [0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><table>\\n  <tbody><tr>\\n    <td>How to Scrape Things</td>\\n    <td>Some Supplemental Materials</td>\\n    <td>By Jonathan Soma</td>\\n  </tr>\\n</tbody></table></body></html>'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/single-table-row.html\")\n",
    "await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr>\n",
       " <td>How to Scrape Things</td>\n",
       " <td>Some Supplemental Materials</td>\n",
       " <td>By Jonathan Soma</td>\n",
       " </tr>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "doc_3 = BeautifulSoup(html)\n",
    "doc_3\n",
    "\n",
    "doc_3 = doc_3.select(\"tr\")\n",
    "doc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Scrape Things',\n",
       "  'subhead': 'Some Supplemental Materials',\n",
       "  'byline': 'By Jonathan Soma'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start off with ZERO ROWS OF DATA\n",
    "# an EMPTY LIST\n",
    "all_data = []\n",
    "\n",
    "for row in doc_3:\n",
    "    book = {}\n",
    "    \n",
    "    # print out the html or the .text\n",
    "    # .title .name-div p means\n",
    "    # \"a p tag inside of something with the class of name-div\n",
    "    # inside of something with the class of .title\"\n",
    "    book['title'] = row.find_all(\"td\")[0].text\n",
    "    book['subhead'] = row.find_all(\"td\")[1].text\n",
    "    book['byline'] = row.find_all(\"td\")[2].text\n",
    "    all_data.append(book)\n",
    "\n",
    "len(all_data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/multiple-table-rows.html, printing out each title, subhead, and byline.\n",
    "\n",
    "> You won't use pandas for this one, either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html><head></head><body><table>\\n  <tbody><tr>\\n    <td>How to Scrape Things</td>\\n    <td>Some Supplemental Materials</td>\\n    <td>By Jonathan Soma</td>\\n  </tr>\\n  <tr>\\n    <td>How to Scrape Many Things</td>\\n    <td>But, Is It Even Possible?</td>\\n    <td>By Sonathan Joma</td>\\n  </tr>\\n  <tr>\\n    <td>The End of Scraping</td>\\n    <td>Let's All Use CSV Files</td>\\n    <td>By Amos Nathanos</td>\\n  </tr>\\n</tbody></table></body></html>\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless = False)\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/lede/static/multiple-table-rows.html\")\n",
    "await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr>\n",
       " <td>How to Scrape Things</td>\n",
       " <td>Some Supplemental Materials</td>\n",
       " <td>By Jonathan Soma</td>\n",
       " </tr>]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "doc_6 = BeautifulSoup(html)\n",
    "doc_6\n",
    "\n",
    "doc_6 = doc_6.select(\"tr\")\n",
    "doc_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<td>How to Scrape Things</td>,\n",
       "  <td>Some Supplemental Materials</td>,\n",
       "  <td>By Jonathan Soma</td>]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "for row in doc_6:\n",
    "    data = row.find_all(\"td\")\n",
    "    all_data.append(data)\n",
    "all_data\n",
    "\n",
    "#once I put .text after (\"td\"), I get an error message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html, creating a list of dictionaries.\n",
    "\n",
    "> Don't use pandas here, either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head></head><body><table id=\"booklist\">\\n  <tbody><tr>\\n    <td>How to Scrape Things</td>\\n    <td>Some Supplemental Materials</td>\\n    <td>By Jonathan Soma</td>\\n  </tr>\\n  <tr>\\n    <td>How to Scrape Many Things</td>\\n    <td>But, Is It Even Possible?</td>\\n    <td>By Sonathan Joma</td>\\n  </tr>\\n  <tr>\\n    <td>The End of Scraping</td>\\n    <td>Let\\'s All Use CSV Files</td>\\n    <td>By Amos Nathanos</td>\\n  </tr>\\n</tbody></table></body></html>'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/lede/static/the-actual-table.html\")\n",
    "await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr>\n",
       " <td>How to Scrape Things</td>\n",
       " <td>Some Supplemental Materials</td>\n",
       " <td>By Jonathan Soma</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>How to Scrape Many Things</td>\n",
       " <td>But, Is It Even Possible?</td>\n",
       " <td>By Sonathan Joma</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>The End of Scraping</td>\n",
       " <td>Let's All Use CSV Files</td>\n",
       " <td>By Amos Nathanos</td>\n",
       " </tr>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "doc_7 = BeautifulSoup(html)\n",
    "\n",
    "doc_7 = doc_7.select(\"tr\")\n",
    "doc_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Scrape Things',\n",
       "  'subhead': 'Some Supplemental Materials',\n",
       "  'byline': 'By Jonathan Soma'},\n",
       " {'title': 'How to Scrape Many Things',\n",
       "  'subhead': 'But, Is It Even Possible?',\n",
       "  'byline': 'By Sonathan Joma'},\n",
       " {'title': 'The End of Scraping',\n",
       "  'subhead': \"Let's All Use CSV Files\",\n",
       "  'byline': 'By Amos Nathanos'}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "\n",
    "for row in doc_7:\n",
    "    actual_table = {}\n",
    "    \n",
    "    # print out the html or the .text\n",
    "    # .title .name-div p means\n",
    "    # \"a p tag inside of something with the class of name-div\n",
    "    # inside of something with the class of .title\"\n",
    "    actual_table['title'] = row.find_all(\"td\")[0].text\n",
    "    actual_table['subhead'] = row.find_all(\"td\")[1].text\n",
    "    actual_table['byline'] = row.find_all(\"td\")[2].text\n",
    "    all_data.append(actual_table)\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Scraping multiple table rows into a list of dictionaries\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html, creating a pandas DataFrame.\n",
    "\n",
    "> There are two ways to do this one! One uses just pandas, the other one uses the result from Part 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subhead</th>\n",
       "      <th>byline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Some Supplemental Materials</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title                      subhead            byline\n",
       "0       How to Scrape Things  Some Supplemental Materials  By Jonathan Soma\n",
       "1  How to Scrape Many Things    But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping      Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Scraping into a file\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/lede/static/the-actual-table.html and save it as `output.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
